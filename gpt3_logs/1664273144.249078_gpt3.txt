Summarize the following paper for literature review.


PAPER:
descent/ascent [8], embed the modules with accurate models into DNN-based systems, and rst use traditional model-based solutions to initialize and then apply DNNs to rene [16], [17]. Despite that the basic idea is general and useful, mathematical models are problem specic, and hence the solutions with model-based DNNs have to be developed on a case by case basis. Nonetheless, the two branches of research that are respectively priori-based and model-driven, are complementary rather than mutual exclusion. Given the great potential of deep learning in beyond 5G/6G cellular networks, it is natural to raise the following question: are there any general priors in wireless tasks? If yes, how to design DNN architecture by incorporating the priors? Each task corresponds to a specic relation (i.e., a function). In many wireless tasks, the relation between the concerned solutions and the relevant parameters satises a common property: permutation invariance. For example, if the channel gains of multiple users permute, then the resources allocated to the users permute accordingly. This is because the resource allocated to a user depends on its own channel but not on the permutation of other users channels [4][6], [10]. While the property seems obvious, the way to exploit the knowledge is not straightforward. In this paper, we strive to demonstrate how to reduce training complexity by harnessing such general knowledge . We consider two kinds of permutation invariance properties, which widely exist in wireless tasks. For the tasks satisfying each kind of property, we nd a DNN with special architecture to represent the relation between the solution and the concerned parameters, referred to as permutation invariant DNN (PINN) , where majority of the model parameters are identical. Different from CNN and RNN that exploit the characteristic of data, which is the input of the DNN, PINN exploits the characteristic of tasks, which decide the input-output relation. The architecture of PINN offers the exibility in applying to different input data dimension. By jointly trained with a small size DNN that captures the impact of the input dimension, the constructed DNNs can adapt to wireless systems with different scales (e.g., with time-varying January 31, 2020 DRAFT 3 number of users). Except the DNN architecture, we show that the property can also be used to generate labels for supervised learning. Simulation results show that much fewer samples and much lower computational complexity are required for training the constructed PINNs to achieve a given performance, and the majority of labels can be generated with the permutation invariance property. The proposed PINNs can be applied for a broad range of wireless tasks, including but not limited to the tasks in [3][6], [8], [10][12]. The major contributions are summarized as follows. We nd the sufcient and necessary conditions for tasks to satisfy two kinds of permutation invariant properties. For each kind of tasks, we construct a DNN architecture whose input- output relationship satises the permutation invariance property. The constructed PINNs are applicable to both unsupervised and supervised learning. We show how the PINNs can adapt to different input data dimension by introducing a factor to characterize the impact of the scale of a wireless system. In training phase, the complexities can be reduced by training DNNs with small size. In operation phase, the trained DNN can be adaptive to the input with time-varying dimension. We take predictive resource allocation and interference coordination as examples to illustrate how the PINNs can be applied to unsupervisely and supervisely learn the two kinds of permutation invariant functions, respectively. Simulation results demonstrate that the constructed PINNs can reduce the sample and computational complexities remarkably compared to the non-structural FC-DNN with same performance. Notations :Efgdenotes mathematical expectation, kk denotes two-norm, kk 1denotes the summation of the absolute values of all the elements in a vector or matrix, and ()Tdenotes transpose, 1denotes a column vector with all elements being 1,0denotes a column vector or a matrix with all elements being 0. The rest of the paper is organized as follows. In section II, we introduce two permutation invariance properties and construct two PINNs, and illustrate how the PINNs can adapt to the input dimension. In section III and IV, we present two case studies. In section V, we show that the PINNs can reduce training complexity, and illustrate that the properties can also be used for dataset augmentation. In section VI, we provide the concluding remarks. January 31, 2020 DRAFT 4 II. DNN FOR TASKS WITH PERMUTATION INVARIANCE In this section, we rst introduce two kinds of relationships (mathematically, two kinds of functions) with permutation invariant property, which are widely existed in wireless communi- cation tasks. For each relationship, we demonstrate how to construct a parameter sharing DNN satisfying the property. Then, we show how to make the constructed DNN adaptive to the scale of wireless networks. A. Denition and Example Tasks For many wireless tasks such as resource allocation and transceiver design, the optimized policy that yields the solution (represented as column vector ywithout the loss of generality) from environment parameters (represented as vector xor matrix X) can be expressed as a function y=f(x)ory=f(X). Both yandxare composed of Kblocks, i.e., y= [yT 1;;yT K]T, x= [xT 1;;xT K]T, and Xis composed of K2blocks, i.e., X=2 66664x11 x1K ......... xK1xKK3 77775; (1) where the block ykandxkcan either be a scalar or a column vector, k= 1;;K, and the block xmncan be a scalar, vector or matrix, m;n = 1;;K. A property is widely existed in the optimized policies f()for wireless problems: one- dimensional (1D) permutation invariance of y=f(x)and two-dimensional (2D) permutation invariance of


LITERATURE REVIEW SUMMARY:

==========

The paper discusses the potential of using deep learning to reduce training complexity for wireless tasks by harnessing permutation invariance properties. Two permutation invariant properties are introduced, and corresponding DNN architectures are proposed. It is shown how the PINNs can adapt to different input data dimension. The proposed PINNs are demonstrated to reduce the sample and computational complexities compared to the non-structural FC-DNN with same performance.