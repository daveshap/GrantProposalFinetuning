Summarize the following paper for literature review.


PAPER:
Conf. Comput. Vis. Pattern Recognit. , vol. 07 -12-June-2015, pp.  815823, Mar. 2015, doi: 10.1109/CVPR.2015.7298682.   [2] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ImageNet Classification  with Deep Convolutional Neural Networks, Commun. ACM , vol. 60, no.  6, 2017, doi: 10.1145/3065386.   [3] K. Simonyan and A. Zisserman, Very Deep Convolutional Networks for  Large -Scale Image Recognition, 3rd Int. Conf. Learn. Represent. ICLR  2015 - Conf. Track Proc. , Sep. 2014, doi: 10.48550/arxiv.1409.1556.   [4] K. He, X. Zhang, S. Ren, and J. Sun, Deep Residual Learning for Image  Recognition, Proc. IEEE Comput. Soc. Conf. Comput. Vis . Pattern  Recognit. , vol. 2016 -December, pp. 770 778, Dec. 2015, doi:  10.48550/arxiv.1512.03385.   [5] M. Peemen, A. A. A. Setio, B. Mesman, and H. Corporaal, Memory - centric accelerator design for convolutional neural networks, 2013 IEEE  31st Int. Conf. Co mput. Des. ICCD 2013 , pp. 13 19, 2013, doi:  10.1109/ICCD.2013.6657019.   [6] C. Zhang, P. Li, G. Sun, Y. Guan, B. Xiao, and J. Cong, Optimizing  FPGA -based accelerator design for deep convolutional neural networks,  in FPGA 2015 - 2015 ACM/SIGDA Internationa l Symposium on Field - Programmable Gate Arrays , 2015, pp. 161 170, doi:  10.1145/2684746.2689060.   [7] M. Motamedi, P. Gysel, V. Akella, and S. Ghiasi, Design space  exploration of FPGA -based Deep Convolutional Neural Networks, Proc.  Asia South Pacific Des. Autom. Conf. ASP -DAC , vol. 25 -28-Janu, pp.  575580, 2016, doi: 10.1109/ASPDAC.2016.7428073.   [8] Y. Guan, N. Xu, C. Zhang, Z. Yuan, and J. Cong, Using Data  Compression for Optimizing FPGA -Based Convolutional Neural  Network Accelerators, Lect. Notes Comput . Sci. (including Subser. Lect.  Notes Artif. Intell. Lect. Notes Bioinformatics) , vol. 10561 LNCS, pp. 14  26, 2017, doi: 10.1007/978 -3-319-67952 -5_2.  [9] W. Hu, S. Chen, Z. Li, T. Liu, and Y. Li, Data optimization cnn  accelerator design on fpga, Proc. - 2019 IEEE Intl Conf Parallel Distrib.  Process. with Appl. Big Data Cloud Comput. Sustain. Comput. Commun.  Soc. Comput. Networking, ISPA/BDCloud/SustainCom /SocialCom 2019 ,  pp. 294 299, 2019, doi: 10.1109/ISPA -BDCloud -SustainCom - SocialCom48970.2019.00051.   [10] K. Bjerge, J. H. Schougaard, and D. E. Larsen, A scalable and efficient  convolutional neural network accelerator using HLS for a system -on-chip  design , Microprocess. Microsyst. , vol. 87, p. 104363, Nov. 2021, doi:  10.1016/j.micpro.2021.104363.   [11] M. Blott et al. , FinN -R: An end -to-end deep -learning framework for fast  exploration of quantized neural networks, ACM Trans. Reconfigurable  Technol. Syst. , vol. 11, no. 3, 2018, doi: 10.1145/3242897.   [12] S. I. Venieris and C. S. Bouganis, FpgaConvNet: Mapping Regular and  Irregular Convolutional Neural Networks on FPGAs, IEEE Trans.  Neural Networks Learn. Syst. , vol. 30, no. 2, pp. 326 342, 2019, doi:  10.1109/TNNLS.2018.2844093.   [13] K. Guo et al. , Angel -Eye: A complete design flow for mapping CNN  onto embedded FPGA, IEEE Trans. Comput. Des. Integr. Circuits Syst. ,  vol. 37, no. 1, pp. 35 47, Jan. 2018, doi: 10.1109/TCAD.2017.2705069.   [14] H. Sharma et al. , From high -level deep neural models to FPGAS, in  Proceedings of the Annual International Symposium on  Microarchitecture, MICRO , Dec. 2016, vol. 2016 -Decem, doi:  10.1109/MICRO.2016.7783720.   [15] PYNQ - Python productivity for Zynq - Home . http://www.pynq.io/  (accessed Nov. 17, 2021).   [16] 10. Model Zoo  PyTorch/Serve master documentation.  https://pytorch.org/serve/model_zoo.html (accessed May 21, 2022).   [17] Azzam -Alhussain/CNNs -Template: This repository will contain an  open -source c ode to our efficient and scalable accelerator template -based  design after the paper is being accepted. Stay tuned.  https://github.com/Azzam -Alhussain/CNNs -Template


LITERATURE REVIEW SUMMARY:

==========

The paper discusses various ways to optimize FPGA-based accelerator design for deep convolutional neural networks (CNNs). The authors compare and contrast different approaches, including data compression, HLS, and model compression. They conclude that data compression is the most effective way to reduce the size of the CNN accelerator.