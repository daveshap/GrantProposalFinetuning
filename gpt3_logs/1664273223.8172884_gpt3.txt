Summarize the following paper for literature review.


PAPER:
problem with class 1 characterizing abnormality and class 0 for normality). To prevent this from happening, during the training process the samples of each class are weighted inversely according to the class label distribution. That means that observations of less dominant classes have a higher chance to be chosen and inuence the feature extraction process. It is referred to this procedure in the following as inverse weighting . To summarize, it is something that inuences the training process of random forests to overcome class imbalances, but does not inuence the logic translation itself. Logic Translation of a Single Decision Tree - Main Principle The nal leaves of the decision tree implementations model class probabilities for each of the class labels, according to the distribution of observed training examples that fell into the nodes bin. Instead of traversing the tree from top to bottom and either going left or right at each branch according to the decision rule, in the logic the tree is propagated backwards from bottom to top. This is done for the reason that a nal wire is needed at which the signal, i.e. nal class prediction, can be propagated to the next logic module. The simplest way to do so is the root node. Hence, the decision rules are evaluated the opposite way and the class probabilities are propagated upwards to the root node according to the decision rules at the branches. This means that a cascade of comparators does the job of evaluating the decision rules and drives a cascade of multiplexers which select the corresponding class probabilities that need to be propagated. Thus, when executing the logic as simulation on quantized test signals, the root nodes wires receive signals that model the class probabilities of the trees node at which one lands when following the learned decision rules. This is the main principle of translating a single decision tree to logic. However, there are detailed differences in what is propagated and how the predictions of multiple trees are handled in the logic to form a whole prediction of the forest. The differences are dependent on whether a bitwise or none-bitwise option of logic creation is chosen. This is explained in the following. Bitwise Training An option that inuences both the training and the translation is the option for bitwise training . Within that, the neural networks activations are quantized to binary strings. Then, a random forest is trained on each bit 0j < m separately and solves a binary classication problem. Note that subsets of the quantized activation data have to be formed that always extract the jthbit of each feature and label. Thus, the decision trees also receive samples with each feature being a single-bit, as illustrated in Figure 4. This procedure is also done for simplifying the logic itself. When only using binary data, the random forest problem is simplied to thresholds of 0:5at the branches as there are only two possible values 0and1(or any other xed threshold between 0 and 1). What remains to be learned by the random forests is which feature is relevant for the classication. This is equivalent to learning a removal of unimportant" variables and hence, to a dont-care-based dependency elimination that might be able to drive generalization as described in Section 4.1. x1   :    1.6    x2   :    1.2x3   :    2.3x4   :   -0.7Example ObservationFrom Activations:x1   :   011001 (25)    x2   :   010011 (19)x3   :   011111 (31)x4   :   110101 (53)Quantization - 6 total bits, 4 fractional bits:Binary String (Unsigned Integer)x1   :   011001    x2   :   010011x3   :   011111x4   :   110101Selection of j-th Bit: j = 3x1   :   0    x2   :   0x3   :   1x4   :   1Training Data Observation Figure 4: Example of how the jthbit of each quantized activation is selected to form the features for bitwise training and logic translation. In this setting, the probabilities of a prediction whether the bit should be classied as 1 or 0 are propagated. Hence, the root node of a decision tree for a single-bit classication has two wires of m bits that model the probability of the sample being class 0 and class 1. The logic of an exemplary single decision tree under bitwise training is visualized in Figure 4. The probability wires at each decision trees root node are used to form overall class probabilities of the random forest. This is done by summing up all propagated probabilities of class 0 and class 1 15 class: 0 class: 1x1>=0.5x1<0.5x3<0.5x4>=0.5[0.75 ; 0.25][0.4 ; 0.6][0.2 ; 0.8][0.0 ; 1.0][0.4 ; 0.6] >=MUXx10.5x3x4>=0.50.750.40.20.250.60.80.01.00    00    11    01    10    00    11    01    1MUXMUXx3/x4x1 x3/x4x1prob. class 0prob. class 1x1   x2   x3   x4 0     0     1     1Example Observation: Root Node WiresFigure 5: Overview of the logic of a single decision tree of depth 2 for 4 data features under bitwise training . The nal leaves model class probabilities of the binary classication problem. The logic is implemented as a cascade of comparators and multiplexers. across the trees with an accumulator circuit. Same as in the setting of the arithmetic circuit (Section 7.2.1) the accumulator is assumed to be of size 3mbits to prevent overow. In the following it is referred to this summing up operation as a majority vote because each decision tree contributes equally to the class probabilities. To then propagate the actual single-bit class prediction of the forest, a logic operation follows that mimics the argmax-function via a comparator that compares the summed-up probabilities. The comparator drives a multiplexer which chooses between a constant 1-bit signal of 1 or 0. To form the nal signal that is provided as input to the next logic module, all single bit predictions of the mforests are again concatenated to form the m-bit signal. Both the majority vote and the argmax operation are visualized in Figure 6. Inuence of the Argmax-Function in the Bitwise Training Setting It should be highlighted that


LITERATURE REVIEW SUMMARY:

==========

The paper discusses the use of inverse weighting to prevent class imbalance when training a random forest, and the use of bitwise training to simplify the logic of the forest. The argmax function is used to determine the final class prediction of the Forest.