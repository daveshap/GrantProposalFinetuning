Summarize the following paper for literature review.


PAPER:
1,279,097 1,281,932.67 (2542.51) 188 188.00 (0.00) 34.63 33.67 (0.85) 4 15 Yes 1,698,388 1,694,545.00 (3463.69) 200 200.00 (0.00) 34.56 33.02 (1.34) 4 15 No 278,228 278,089.00 (122.65) 217 217.00 (0.00) 34.29 33.33 (0.85) 2 20 No 162,708 162,656.33 (46.74) 179 179.00 (0.00) 34.29 33.76 (0.68) 2 5 Yes 63,876 64,050.67 (214.44) 134 134.00 (0.00) 34.15 32.97 (1.35) 3 20 No 225,517 225,617.00 (193.47) 200 200.33 (0.58) 33.95 32.88 (0.94) 3 15 No 217,333 217,788.67 (446.78) 195 195.00 (0.00) 33.95 33.31 (0.55) 2 5 No 41,774 41,771.67 (2.08) 133 133.00 (0.00) 33.95 33.90 (0.04) 2 15 Yes 874,693 874,090.67 (2911.61) 176 176.00 (0.00) 33.81 33.76 (0.08) 4 5 No 52,981 52,983.67 (2.31) 177 177.00 (0.00) 33.67 33.63 (0.03) 4 20 No 291,068 291,132.67 (335.70) 222 222.00 (0.00) 33.61 33.04 (0.55) 2 10 Yes 545,141 544,665.33 (1056.15) 156 156.00 (0.00) 33.54 32.31 (1.67) 4 20 Yes 1,609,243 1,605,345.67 (3621.95) 218 217.67 (0.58) 33.47 32.06 (2.32) 3 10 No 139,840 139,775.67 (178.42) 175 175.00 (0.00) 33.40 33.29 (0.10) 2 15 No 155,526 155,460.67 (139.03) 173 173.00 (0.00) 32.86 32.56 (0.26) Table 14: Results of the random forest logic on the "V ACS Model 2" setup for a quantization scheme of 32 total bits and 16 fractional bits . EstimatorsMax. DepthBitwiseAIG Nodes (Top1)AIG Nodes (TopN)AIG Levels (Top1)AIG Levels (TopN)Accuracy (Top-1)Accuracy (Top-N) 4 15 Yes 2,097,694 2,095,119.33 (5562.45) 373 373.33 (0.58) 39.39 35.26 (3.76) 2 10 No 254,579 254,570.67 (14.43) 354 354.00 (0.00) 38.91 38.91 (0.00) 4 20 No 584,884 584,902.33 (799.66) 527 527.00 (0.00) 37.07 36.51 (0.54) 3 20 Yes 1,621,931 1,625,149.00 (3028.34) 361 361.00 (0.00) 36.53 34.94 (1.88) 3 10 No 280,859 280,859.00 (0.00) 424 424.00 (0.00) 36.46 36.46 (0.00) 3 10 Yes 1,631,324 1,623,684.67 (9490.92) 361 360.67 (0.58) 36.39 34.76 (1.54) 3 20 No 503,445 504,230.67 (698.40) 457 457.00 (0.00) 36.39 35.17 (1.47) 2 15 No 334,551 334,086.67 (540.73) 373 373.00 (0.00) 36.19 34.97 (1.09) 2 20 No 420,111 419,133.67 (1329.29) 387 387.00 (0.00) 36.12 35.76 (0.46) 3 15 No 390,306 389,998.67 (321.87) 443 443.00 (0.00) 35.99 35.65 (0.59) 3 5 Yes 399,979 398,883.33 (1068.15) 351 350.33 (0.58) 35.58 34.17 (1.73) 4 20 Yes 2,106,129 2,099,547.00 (6646.41) 374 373.67 (0.58) 35.58 34.70 (0.85) 4 15 No 436,586 437,421.67 (758.03) 513 513.00 (0.00) 35.24 34.20 (0.91) 2 20 Yes 1,148,673 1,145,816.00 (3890.61) 349 349.00 (0.00) 34.97 34.40 (0.52) 4 10 No 304,540 304,540.00 (0.00) 494 494.00 (0.00) 34.35 34.35 (0.00) 4 10 Yes 2,087,126 2,091,456.00 (6062.00) 372 372.67 (0.58) 34.29 33.56 (0.79) 2 10 Yes 1,141,887 1,145,072.33 (2923.82) 349 349.00 (0.00) 33.95 33.04 (1.08) 2 15 Yes 1,148,683 1,148,573.00 (1836.47) 349 348.67 (0.58) 33.81 33.27 (0.77) 2 5 No 198,386 198,386.00 (0.00) 334 334.00 (0.00) 33.81 33.81 (0.00) 3 15 Yes 1621829 1,619,763.00 (5261.56) 361 361.33 (0.58) 33.74 32.31 (1.46) 4 5 Yes 478459 476,360.67 (1914.80) 362 361.67 (0.58) 33.20 32.43 (0.70) 2 5 Yes 321984 321,670.67 (271.89) 339 338.33 (0.58) 32.79 32.18 (0.69) 3 5 No 203821 203,821.00 (0.00) 404 404.00 (0.00) 30.95 30.95 (0.00) 4 5 No 209776 209,766.67 (16.17) 474 474.00 (0.00) 29.80 29.75 (0.08) 24 7.4.2 LogicNet Evaluation The tables provided in the following show the results of the logic creation with LogicNets as intermediate steps. They are based on the real-valued neural network baselines dened in Table 1 that served as input to the logic pipeline. In each table, the results are sorted in descending manner according to the top-1 models performance and the column-wise "winner" is marked in bold. Note that if the number of nodes or levels in the AIG is zero, this means that the LogicNets have learned nothing and its corresponding logic has a constant class prediction. Table 15: Results of the LogicNet logic on the "GIB Model 1" setup for a quantization scheme of 7 total bits and 3 fractional bits . Depth Width LUT SizeAIG Nodes (Top1)AIG Nodes (TopN)AIG Levels (Top1)AIG Levels (TopN)Accuracy (Top-1)Accuracy (Top-N) 4 1,000 1 2,508 2,508.00 (0.00) 106 106.00 (0.00) 66.56 66.56 (0.00) 2 100 1 2,508 2,508.00 (0.00) 106 106.00 (0.00) 66.56 66.56 (0.00) 3 50 1 2,477 2,477.00 (0.00) 110 110.00 (0.00) 65.73 65.73 (0.00) 2 500 1 2,477 2,477.00 (0.00) 110 110.00 (0.00) 65.73 65.73 (0.00) 3 1,000 1 2,477 2,477.00 (0.00) 110 110.00 (0.00) 65.73 65.73 (0.00) 2 200 2 7,033 4,688.67 (4,060.50) 112 74.67 (64.66) 50.17 50.11 (0.10) 2 500 3 0 0.00 (0.00) 0 0.00 (0.00) 50.00 50.00 (0.00) 2 1,000 2 0 2,886.00 (4,998.70) 0 37.00 (64.09) 50.00 49.94 (0.10) 4 50 3 0 0.00 (0.00) 0 0.00 (0.00) 50.00 50.00 (0.00) 2 1,000 3 0 0.00 (0.00) 0 0.00 (0.00) 50.00 50.00 (0.00) 4 100 2 0 0.00 (0.00) 0 0.00 (0.00) 50.00 50.00 (0.00) 3 50 2 0 0.00 (0.00) 0 0.00 (0.00) 50.00 50.00 (0.00) 4 100 3 0 0.00 (0.00) 0 0.00 (0.00) 50.00 50.00 (0.00) 3 100 2 0 0.00 (0.00) 0 0.00 (0.00) 50.00 41.66 (9.13) 3 50 3 0 0.00 (0.00) 0 0.00 (0.00) 50.00 50.00 (0.00) 4 200 2 0 0.00 (0.00) 0 0.00 (0.00) 50.00 50.00 (0.00) 4 200 3 0 0.00 (0.00) 0 0.00 (0.00) 50.00 50.00 (0.00) 4 500 2 0 0.00 (0.00) 0 0.00 (0.00) 50.00 50.00 (0.00) 4 500 3 0 0.00 (0.00) 0 0.00 (0.00) 50.00 50.00 (0.00) 4 1,000 2 0 0.00 (0.00) 0 0.00 (0.00) 50.00 50.00 (0.00) 4 50 2 0 0.00 (0.00) 0 0.00 (0.00) 50.00 50.00 (0.00) 4 1,000 3 0 0.00 (0.00) 0 0.00 (0.00) 50.00 50.00 (0.00) 3 1,000 3 0 0.00 (0.00) 0 0.00 (0.00) 50.00 50.00 (0.00) 3 500 2 0 0.00 (0.00) 0 0.00 (0.00) 50.00 50.00 (0.00) 2 50 2 0 3,273.50 (3,779.91) 0 54.50 (62.94) 50.00 45.67 (8.23) 3 100 3 0 0.00 (0.00) 0 0.00 (0.00) 50.00 50.00 (0.00) 2 50 3 0 0.00 (0.00) 0 0.00 (0.00) 50.00 50.00 (0.00) 3 200 2 0 0.00 (0.00) 0 0.00 (0.00) 50.00 50.00 (0.00) 3 200 3 0 0.00 (0.00) 0 0.00 (0.00) 50.00 50.00 (0.00) 2 100 2 0 0.00 (0.00) 0 0.00 (0.00) 50.00 50.00 (0.00) 2 100 3 0 0.00 (0.00) 0 0.00 (0.00) 50.00 50.00 (0.00) 3 500 3 0 0.00 (0.00) 0 0.00 (0.00) 50.00 50.00 (0.00) 2 200 3 0 0.00 (0.00) 0 0.00 (0.00) 50.00 50.00 (0.00) 3 1,000 2 0 0.00 (0.00) 0 0.00 (0.00) 50.00 50.00 (0.00) 2 500 2 7,275 4,842.33


LITERATURE REVIEW SUMMARY:

==========

The paper provides the results of a study on the use of LogicNets as an intermediate step in the creation of logic circuits. The study found that LogicNets can learn to represent the input data well, and that the resulting logic circuits can be accurate.