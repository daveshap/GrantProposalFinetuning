Summarize the following paper for literature review.


PAPER:
 Constructing Deep Neural Networks with a Priori Knowledge of Wireless Tasks Jia Guo and Chenyang Yang Abstract Deep neural networks (DNNs) have been employed for designing wireless systems in many aspects, say transceiver design, resource optimization, and information prediction. Existing works either use the fully-connected DNN or the DNNs with particular architectures developed in other domains. While generating labels for supervised learning and gathering training samples are time-consuming or cost- prohibitive, how to develop DNNs with wireless priors for reducing training complexity remains open. In this paper, we show that two kinds of permutation invariant properties widely existed in wireless tasks can be harnessed to reduce the number of model parameters and hence the sample and computational complexity for training. We nd special architecture of DNNs whose input-output relationships satisfy the properties, called permutation invariant DNN (PINN) , and augment the data with the properties. By learning the impact of the scale of a wireless system, the size of the constructed PINNs can exibly adapt to the input data dimension. We take predictive resource allocation and interference coordination as examples to show how the PINNs can be employed for learning the optimal policy with unsupervised and supervised learning. Simulations results demonstrate a dramatic gain of the proposed PINNs in terms of reducing training complexity. Index Terms Deep neural networks, a priori knowledge, permutation invariance, training complexity I. I NTRODUCTION Deep learning has been considered as one of the key enabling techniques in beyond fth generation (5G) and sixth generation (6G) cellular networks. Recently, deep neural networks (DNNs) have been employed to design wireless networks in various aspects, ranging from signal detection and channel estimation [2], [3], interference management [4], resource allocation [5] [9], coordinated beamforming [10], trafc load prediction [11], and uplink/downlink channel calibration [12], etc, thanks to their powerful ability to learn complex input-output relation [13]. A part of this work is presented in conference version, which has been accepted by IEEE ICC 2020 [1]. January 31, 2020 DRAFTarXiv:2001.11355v1  [cs.LG]  29 Jan 2020 1 For the tasks of transmission scheme or resource allocation, the output is a transceiver or allocated resource (e.g., beamforming vector or transmit power), the input is the environment parameter (e.g., channel gain), and the relation is a concerned policy (e.g., power allocation). For the tasks of information prediction, the relation is a predictor, which depends on the temporal correlation between historical and future samples of a time series (e.g., trafc load at a base station). Existing research efforts focus on investigating what tasks in wireless communications can apply deep learning by considering the fully-connected (FC)-DNN [2], [4], [8], [10], [12], and how deep learning is used for wireless tasks by integrating the DNNs developed in other domains such as computer vision and natural language processing [5], [6], [11]. By nding the similarity between the tasks in different domains, various deep learning techniques have been employed to solve wireless problems. For example, convolutional neural network (CNN) is applied for wireless tasks where the data exhibit spatial correlation, and recurrent neural network (RNN) is applied for information prediction using the data with temporal correlation. Most previous works consider supervised learning. Noticing the fact that generating labels is time-consuming or expensive, unsupervised learning frameworks were proposed for learning to optimize wireless systems recently [5], [9]. Nonetheless, the number of samples required for training in unsupervised manner may still be very high. This impedes the practice use of DNNs in wireless networks where data gathering is cost-prohibitive. Although the computational complexity of off-line training is less of a concern in static scenarios, wireless systems often need to operate in highly dynamic environments, where the channels, number of users, and available resources, etc., are time-varying. Whenever the environment parameters change, the model parameters and even the size of a DNN need to be updated (e.g., the DNN in [10] needs to be trained periodically in the timescale of minutes). Therefore, training DNNs efciently is critical for wireless applications. To circumvent the curse of dimensionality that leads to the unaffordable sample and computational complexity for training, AI society has designed DNN architectures by harnessing general-purposed priors, such that each architecture is applicable for a large class of tasks. One successful example is CNN specialized for vision tasks. By exploiting the knowledge that local groups of pixels in images are often highly correlated, sparse connectivity is introduced in the form of convolution kernels. Furthermore, by exploiting the knowledge that local statistics of images are invariant to positions, parameter sharing is introduced among convolution kernels in each layer [14], [15]. Another example is RNN. Considering the temporal correlation feature of January 31, 2020 DRAFT 2 time series, adjacent time steps are connected with weights, and parameter sharing is introduced among time steps such that the weights between hidden layers are identical [14]. In this way of using a priori knowledge to design the architecture of DNNs, the number of model parameters and hence the training complexity can be reduced. To reduce the training complexity, wireless society promotes model-and-data-driven methodology to combine the well-established communication domain knowledge with deep learning most recently [16], [17]. For instance, the models can be leveraged to generate labeled samples for supervised learning [4], [17], derive gradients to guide the searching directions for stochastic gradient


LITERATURE REVIEW SUMMARY:

==========

This paper discusses how to construct deep neural networks (DNNs) with a priori knowledge of wireless tasks in order to reduce training complexity. The authors show that two kinds of permutation invariant properties widely existed in wireless tasks can be harnessed to reduce the number of model parameters and hence the sample and computational complexity for training. They find special architecture of DNNs whose input-output relationships satisfy the properties, called permutation invariant DNN (PINN), and augment the data with the properties. By learning the impact of the scale of a wireless system, the size of the constructed PINNs can exibly adapt to the input data dimension. The paper takes predictive resource allocation and interference coordination as examples to show how the PINNs can be employed for learning the optimal policy with unsupervised and supervised learning. Simulations results demonstrate a dramatic gain of the proposed PINNs in terms of reducing training complexity.