Summarize the following paper for literature review.


PAPER:
algorithm  in equation . 1 is based on window operation in which   *  weight window is convolved with  *  patch of the  input  pixel of the ( ).  The sum of these operations is  resulted in the ( )  at a particular index. This  straightforward approach has a complex data pattern on the  FPGA which produce poor architecture design. On the contrary,  the FPGA can parallelize the workload, so layers dataflow is  simplified and present low dependency from the on -chip  buffers to the compute unit.   The dataflow modeling of convolutional layer s in Fig. 4 is  shown    as an input neuron ,   as an output  neuron , and  2 as a weight value  where all of them are  cashed into input, output , and weight buffer s for on -chip  processing . First, the data is moved from the input and weight  buffers to the compute unit to perform dot product which and  resulted in  a written values in the output buffer. Then, the  dataflow occurs in a form of vector values across those  channels.  All ( ) values across channels (0, 1, 2,, -1) are  read starting from the index (0,0) to the last index (-1, -1)  and then transferred alongside with the weights values to the  compute unit. After that, the compute unit performs dot product  along the channel dimension of ( )  and resulted in the  output vector (0, 1, 2, ,-1). This process is continuously  repeated for a spatial location  of  *   on ( ) and then  stored on ( ) to achieve high parallelism in the dimension  of input and output channels (, ) and reduce data  dependency for reading and writing among those buffers.      Figure 4. Depiction of Convolution dataflow and computation on FPGA   On the other hand, FC layers working principle is illustrated  in Fig. 5 shown  input neurons and  *  weight values are  cashed on the BRAM buffers. These values are too large to be  processed by the compute unit at once. As a result, another set  of loop tiling/block is introduced for the FC layers which break  (, ) data into smaller ( , ) sizes. The i nput size  and the  weights size  *  are transferred to the compute unit while the  output size  is written back to the output buffer. This  method ensure that the entire input vector is processed  efficiently on the same compute unit.     Figure 5. Depiction of Fully Connected dataflow and computation on FPGA   E. Scalability and Efficiency   The proposed accelerator  utilized  a pre-trained model with  16-bit fixed -point quantization in 2.14 format.  Nevertheless ,  the tiling size of convolution and FC layers d etermines the  optimum performance of the template. We use d a trial-based  method to fine -tune the accelerator for higher efficiency and  better scalability across various SoC-FPGAs  ZYNQ  boards and  CNNs architecture. The parameters were  randomly selected,                                                                                                                *    *  * 2            *           Page | 4     and the design was simulated until the resources and latency  were  met. After sets of trial and error , we found that the tile  factor of  *  affect the performance directly  in which this  proposed template achieved higher  performance  when  is  approximately twice  under resource constraints . As a result,  the accelerator can run advanced CNN architecture such as  ResNet -50, SSD, MobileNets  and YOLO of any version. Since  all of these networks have same  type of layers, the proposed  methodology can map any CNN architecture.   IV. EXPERIMENTAL SETUP AND RESULTS   A. Development Environm ents   The accelerator design was simulated and synthesized using  Vivado High -Level Synthesis (HLS ) (2019.2), and utilized  pre- trained models from PyTorch  Model Zoo  [16]. It was tested  with AlexNet [2], VGG -16 [3], and LeNet  architectures  and can  work with any advanced  CNNs network .  B. Results   The proposed template can operate under  200MHz , and  achieve  superior performance of up to 230 GOP/s . Table 1  reported the resource utilization  and performance  measurement  of AlexNet  network demonstrated on Ultra96 , ZCU104 , and  ZCU102 . The BRAM and DSP are directly dependent  on the  tile size  of  and  , and the number  of dot -products in the  compute unit , while FF and LUT are used to control the logic  gates and state machine for running the loops and control ling  the dataflow.  Finally, our accelerator  was benchmark ed against  the previous development [10] on Ultra96 and achieved higher  performance and lower latency , as reported in Table  2.  Table 1. Resource utilization  and performance measuremment   Device Ultra96  ZCU104  ZCU102   Compute Unit    *  12 x 24  20 x 30  20 x 55   Flip-Flops 23.5k (16%) 46k (10%)  139k (25%)   LUTs 15.6k (22%) 24k (10%)  57k (20%)  BRAM  332 (76%) 594 (95%)  1.7K  (95%)   DSP Slices  334 (92%) 586 (33%)  1.7K  (67%)  Performance  51 GOP/s  107 GOP/s  230 GOP/s   Frequency  169 MHz  198 MHz  167 MHz     Table 2. Benchmark  and comparision    Device Ultra96   Development  Previous method [10] Proposed method   Max frequency  170 MHz 169 MHz   Bit width  16 16  Performance  31 GOP/s  51 GOP/s  Latency ( ms) 4.6 0.174   Power (w)  3.55  4.7  V. CONCLUSION AND FUTURE WORK   The proposed template  efficiently utilized the loop tiling  and dataflow  modeling  for optimized  accelerator design . As a  result, a range of 1.3x - 1.7x higher  performance  was achieved  along  with a minimal  layer  of execution time  when compared  to the previous development  [10]. The analysis and simulation  result s proved to be optimistic and can be  extended to create a  complete framework . This will allow the community to use our open -source project and  search  an efficient implementation  for  real-time applications [17].  REFERENCES   [1] F. Schroff, D. Kalenichenko, and J. Philbin, FaceNet: A Unified  Embedding for Face Recognition and Clustering, Proc. IEEE Comput.  Soc.


LITERATURE REVIEW SUMMARY:

==========

The paper discusses an algorithm for convolutional neural networks that is based on a window operation. The algorithm is designed to be more efficient than previous methods, and is able to achieve higher performance while using less resources. The paper includes results from simulations that show the improved performance of the proposed algorithm.