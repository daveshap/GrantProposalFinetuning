Summarize the following paper for literature review.


PAPER:
parameter  that complicate the accelerator  design . Second, some platforms are limited in scalability due to  their restricted resources. However , the most recent work in line  with our idea  is proposed by Bjerge  et al . [10], which  implemented a  16-bit quantized CNN in 2.14 format of two bits  integer and fourteen bits fractional using a PYNQ  framework   [15]. Nevertheless, this paper  is qualitatively distinct from the  past one s. Our proposed methodology  addressed these issues  and optimized performance, latency, and resource utilization ,  then benchmarked it against the previous development s.    Figure 2. AlexNet network  architecture   III. METHODOLOGY   A. Network Architecture   CNN architecture of AlexNet  [2] shown in Fig. 2 , VGG16   [3], and LeNet are used as a case study for the proposed  template . All of these networks consist of convolutional and FC  layers while the number and size  of these layers vary across  different  operations . The algorithm of convolution al layer s is  illustrated as in the equation  follows :   row {1,2,...,R}   col  {1,2,...,C}   co{1,2,..., }  ci{1,2,...,}  i{3,5,..., }  j{3,5,..., }                  [ ::]=  [  + ][       + ][]  [][][][]            (1)   Rows () and columns () represent the  image/ matrices  size,  while the input channel () and output channel () are the third  dimension s of input feature map ( ) and output feature map  ( ) respectively  and () represents the kernel/filter size.   The total number of convolutional operations are as in the  equation follows:                          .     =2 2                               (2)   On the other hand, the algorithm  of FC layers  is illustrated as in  the equation follows:    co{1,2,..., }  ci{1,2,...,}                                     [co]= [ci]W[co][ci]                             (3)  Where t he operations of FC Layers have one dimensional data  on the input and output neurons which has less processing tasks  compared to the convolutional  layers. The total number of FC  operations is as in the equation follows:                                      .   s=2                                       (4)   And a s previously mentioned, these layers are operationally  expensive in terms of computation and latency. For this reason,  it is pertinent to map them into the PL part of  ZYNQ . The  previous equations represent symmetrical operation while the  dynamics and dataflow of the layers are different. The proposed  methodology finds a common dataflow pattern for an optimized  accelerator design.   B. Loop Tiling Transformation   Equa tion. 1 represent s layer value s of R, C, , and  as  variable  values in which  using these values to perform direct  implantation leads  to an inefficient accelerator design . Hence ,  loop tiling is performed , which convert s loops into fixed  points/blocks . It is also represented in the same algorithm and  uses the tile size as () for , () for , () for , and () for   . Thus , we can transfer a fixed amount of data from the  external memory (DRAM) to on-chip memory  (BRAM). Once  the data is cashed  into BRAM,  fixed  computations are  performed by  the accelerator. On the contrary , loop tiling for  the FC layer s uses the tile size as  () for  and ()for .   In the accelerator design, the FC layers have larger vector  values in the input and output channels compared to the  convolutional layers. Hence, different sizes of tile are chosen.  However, choosing the same size of tiles for the input and  output channels resulted in performance reduction. Lastly, the  overall selection of these tiles can result  in maximum resource  utilization and lower latency.   C. Accelerator Design   The proposed template -based vector design is illustrated in  Fig. 3, shown the loop tiling factors of  ,, , , , and  that  determine the on -chip buffers. Due to t he size difference of  input and output channels in convolutional and FC layers,  diffe rent sizes of tiles are used and prompted to use dedicated  buffers for both types of layers . This method resulted in more  resource utilization and overcome the reading and writing  overhead latency owing  to the multi -dimensional array.  Additionally, those dedicated buffers are being used in the size       Page | 3     of weights in both layers which allow the design to have better   efficiency and lower latency at the cost of more resource  utilization .     Figure 3. The proposed FPGA template -based  design   First, data is cashed on-chip in these buffers using two data  ports where one can be used for  read/write of  ( ) and  ( ) , and the other one can be used only for reading the  weights. Those ports are memory -mapped (M -AXI) , which   enable s a burst transfers  and improv the e xternal memory  bandwidth.  The input and weight buffer are partitioned in  dimension  to improve the design latency as multiple reads and  writes are possible on arrays. This proposed c ompute unit is  designed to do  a dot-product between  and  input/output  neurons respectively.   The template design has a scheduling mechanism among  the interconnect , which orchestrate control logic and dataflow.   Furthermore, the p ing pong data transfer method  is used on the  input, we ight, and output buffers to ensure a simultaneous data  transfer happens from DRAM to on -chip buffers and then from  on-chip buffers to the compute unit.  Lastly, equations of the  total operations performed by IP accelerator (3) and  performance measurement in  Giga Operations per second  (GOP/s) (4) are as follows:      =    =2     2            (3)                   / =     =                      (4)  D. Dataflow Modeling   Dataflow modeling is done concerning the architectural  details of on -chip buffers and the compute unit. Convolutional


LITERATURE REVIEW SUMMARY:

==========

The paper discusses a proposed methodology for designing an accelerator for convolutional and fully connected layers in a CNN. The methodology uses loop tiling to improve performance and resource utilization. The design is benchmarked against previous designs and shown to improve performance.