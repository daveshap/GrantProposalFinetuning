Summarize the following paper for literature review.


PAPER:
 To be presented in the 16th IEEE International Conference on Networking, Architectur e, and Storage  (IEEE NAS 2022),  3-4 October  2022,  and  be published in the  IEEE Xplore Digital library   Page | 1   978-1-6654 -5408 -7/22/$31.00 2022 IEEE  Hardware -Efficient Template -Based Deep CNNs  Accelerator Design  Azzam Alhussain   College of Engineering and Computer Science   University of Central Florida    Orlando, Florida, USA   mr.azzam@knights.ucf.edu  Mingjie Lin   College of Engineering and Computer Science   University of Central Florida    Orlando, Florida, USA   milin@ucf.edu AbstractAcceleration of Convolution al Neural Network  (CNN ) on edge  devices  has recently achieved a remarkable   performance  in image classification and object detection  applications . This paper  propose s an efficient and scalable  CNN - based  SoC-FPGA  accelerator design  that takes pre-trained   weights with a 16-bit fixed -point quantization  and target   hardware specification to generate  an optimized template  capable  of achieving higher performance versus resource utilization trade - off. The template  analyz ed the computational workload, data  dependency,  and external  memory bandwidth  and utilized  loop  tiling  transformation  along wit h dataflow modeling  to convert  convolution al and fully connected layer s into vector multiplication  between  input and output feature maps , which result ed in a single  comput e unit on-chip. Furthermore, the accelerator was examined   among  AlexNet, VGG 16, and LeNet networks  and ran at 200 -MHz  with a peak performance of 230 GOP/s depending on ZYNQ   board s and state -space exploration of different  compute unit   configurations during simulation and synthesis. Lastly, our  proposed methodology was benchmarked against the previous  development  on Ultra96  for higher performance measurement .   Keywords CNN , FPGA , Deep Learning , Accelerator design   I. INTRODUCTION   Convolution al Neural Network (CNN) ha s achieved state- of-the-art accuracy in human -based image classification . For  instance , Fac eNet [1] has an astonishing accuracy of 99% for  human fac e recognition  in real -time applications. Beside s,  CNN s have different architecture design s, such as AlexNet [2],  VGG16 [3], and ResNet  [4], but mainly consist of  convolution al, ReLU, pooling,  flatten, Fully Connected (FC) ,  and SoftMax layers as illustrated in Fig. 1. Krizhevsky  et al.  [2]  has shown that 95% of the intensive computation  workload s are  comprise d among  convolution al and FC layers. Although the  dimension and dataflow  are considered the o nly difference  between these layer s, in which FC  layers  have a one- dimensional vector and convolution al layers  usually have a 3- dimensional  array. Nevertheless,  advance d CNN s architecture   become s a challenging task and effort -hungry process to  scale   their workloads efficiently into one solution .  However, CNN 's performance has increased dramaticall y  the last  couple of years  at the cost of huge  computational  workload  that is challenging for the Central Processing Units  (CPUs) . Therefore, Graphical Processing Units (GPUs) have  been the most suitable integration unit to implement Deep  Neural Network s (DNNs) on floating -points due to their highly  optimized parallel processing  cores , but at a significant ly higher  energy consumption  which is not suitable for mobile s' and  robot s' vision application s. Alternatively , Field -Programmable  Gate Array ( FPGA ) is an attractive  acceleration  platform  which  has a customizable internal structure that optimize  the  computation  as GPUs and  implement s computer vision  algorithms  efficiently under fixed -points  with lower  energy   consumption  and minimum processing delay .     Figure 1. Traditional CNN a rchitecture   The aim of this research  is to find  common patterns among   two algorithms , creat e an HW/SW partitioning scheme, then  develop an  efficient and scalable accelerator on the  Programmable Logic ( PL) to compute the intensive operations   of convolution and FC layers  and gain  higher performance . On  the contrary , network initialization, pooling, normalization ,  SoftMax , and other  layers are performed  on the Processing  System  (PS). The contribution s of this paper  are as follow s:    Compute d convolution al and FC layers operation s in  vector multiplication on a single on -chip compute  unit.    Utilized l oop tiling transformation efficiently to construct  the IP accelerator core.    Demonstrated  that the proposed methodology achieve d  superior performance  up to 230  GOP/s  under 200 -MHz  with minimum data execution  time.  The rest of this paper is organized as follows: Section II  provides background on Deep CNNs architecture  and its  hardware implementation. Section III discusses the  methodology of the network architecture and the accelerator  mechanism. Section IV describes the experimental tools and  presents the results, and Section V concludes the paper.         Page | 2     II. BACKGROUND   DNNs acceleration and deployment on FPGAs is an active  research area  compared with other edge computing platforms  due to its efficiency and scalability . For the efficiency side,  most researchers focused  on optimizing the comput ation engine  of the systolic array and neglected the external memory  bandwidth , which resulted in lower performance.  For this  reason , Chakradhar  at el . [5] discussed a separate bank of  scratchpad DRAM to improve the computation . Authors in  [6],  [7] analyze d the data access patterns and use d on-chip buffers  to store  the tiles of external data  and maximize the reusability  of BRAM . Additionally, Guan et at.  [8] used data compression ,  while Hu et al.  [9] propo sed data quantization to  overcome this  obstacle.    Researchers in [10]-[14] reported major issues  on the  scalability side . First, different CNNs architecture have  different layers'


LITERATURE REVIEW SUMMARY:

==========

The paper discusses the design of an efficient and scalable CNN-based SoC-FPGA accelerator that takes pre-trained weights with a 16-bit fixed-point quantization and target hardware specification to generate an optimized template. The template analyzes the computational workload, data dependency, and external memory bandwidth and utilizes loop tiling transformation along with dataflow modeling to convert convolutional and fully connected layers into vector multiplication between input and output feature maps, which resulted in a single compute unit on-chip. Furthermore, the accelerator was examined among AlexNet, VGG 16, and LeNet networks and ran at 200-MHz with a peak performance of 230 GOP/s depending on ZYNQ boards and state-space exploration of different compute unit configurations during simulation and synthesis. Lastly, the proposed methodology was benchmarked against the previous development on Ultra96 for higher performance measurement.